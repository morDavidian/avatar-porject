{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as TF\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "x_train_data = pd.read_csv(\".\\\\samples\\\\train\\\\jupyter_landmarks.csv\",\n",
    "                           usecols = range(1, 209))\n",
    "\n",
    "y_train_data = pd.read_csv(\".\\\\samples\\\\train\\\\unity_blendshapes.csv\",\n",
    "                           usecols = range(2, 74))\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_data.values, requires_grad=True).float()\n",
    "\n",
    "for i in range(x_train_tensor.shape[1]):\n",
    "    sdev_norm = torch.std(x_train_tensor[:, i])\n",
    "    sdev_norm = 0.001 if sdev_norm == 0 else sdev_norm\n",
    "    x_train_tensor[:, i] = (x_train_tensor[:, i] - torch.mean(x_train_tensor[:, i])) / sdev_norm\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_data.values, requires_grad=True).float()\n",
    "\n",
    "\n",
    "for i in range(y_train_tensor.shape[1]):\n",
    "    sdev_norm = torch.std(y_train_tensor[:, i])\n",
    "    sdev_norm = 0.001 if sdev_norm == 0 else sdev_norm\n",
    "    y_train_tensor[:, i] = (y_train_tensor[:, i] - torch.mean(y_train_tensor[:, i])) / sdev_norm\n",
    "\n",
    "\n",
    "\n",
    "x_test_data = pd.read_csv(\".\\\\samples\\\\test\\\\jupyter_landmarks.csv\",\n",
    "                           usecols = range(1, 209))\n",
    "\n",
    "y_test_data = pd.read_csv(\".\\\\samples\\\\test\\\\unity_blendshapes.csv\",\n",
    "                           usecols = range(2, 74))\n",
    "\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test_data.values, requires_grad=True).float()\n",
    "\n",
    "for i in range(x_test_tensor.shape[1]):\n",
    "    sdev_norm = torch.std(x_test_tensor[:, i])\n",
    "    sdev_norm = 0.001 if sdev_norm == 0 else sdev_norm\n",
    "    x_test_tensor[:, i] = (x_test_tensor[:, i] - torch.mean(x_test_tensor[:, i])) / sdev_norm\n",
    "\n",
    "y_test_tensor = torch.tensor(y_test_data.values, requires_grad=True).float()\n",
    "\n",
    "sdev_history = []\n",
    "mean_history = []\n",
    "\n",
    "\n",
    "for i in range(y_test_tensor.shape[1]):\n",
    "    sdev_norm = torch.std(y_test_tensor[:, i])\n",
    "    sdev_norm = 0.001 if sdev_norm == 0 else sdev_norm\n",
    "    \n",
    "#     sdev_history.append(sdev_norm)\n",
    "#     mean_history.append(torch.mean(y_train_tensor[:, i]))\n",
    "    \n",
    "    y_test_tensor[:, i] = (y_test_tensor[:, i] - torch.mean(y_test_tensor[:, i])) / sdev_norm\n",
    "\n",
    "\n",
    "class LandmarksDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "train_dataset = LandmarksDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = LandmarksDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3103 (0%)]\tLoss: 1.018746\n",
      "Train Epoch: 0 [500/3103 (16%)]\tLoss: 0.881655\n",
      "Train Epoch: 0 [1000/3103 (32%)]\tLoss: 0.963214\n",
      "Train Epoch: 0 [1500/3103 (48%)]\tLoss: 0.876432\n",
      "Train Epoch: 0 [2000/3103 (63%)]\tLoss: 0.947020\n",
      "Train Epoch: 0 [2500/3103 (79%)]\tLoss: 0.885221\n",
      "Train Epoch: 0 [3000/3103 (95%)]\tLoss: 0.926224\n",
      "Train Epoch: 1 [0/3103 (0%)]\tLoss: 0.924159\n",
      "Train Epoch: 1 [500/3103 (16%)]\tLoss: 0.869222\n",
      "Train Epoch: 1 [1000/3103 (32%)]\tLoss: 0.908651\n",
      "Train Epoch: 1 [1500/3103 (48%)]\tLoss: 0.806775\n",
      "Train Epoch: 1 [2000/3103 (63%)]\tLoss: 0.742221\n",
      "Train Epoch: 1 [2500/3103 (79%)]\tLoss: 0.882386\n",
      "Train Epoch: 1 [3000/3103 (95%)]\tLoss: 0.754821\n",
      "Train Epoch: 2 [0/3103 (0%)]\tLoss: 0.802896\n",
      "Train Epoch: 2 [500/3103 (16%)]\tLoss: 0.740963\n",
      "Train Epoch: 2 [1000/3103 (32%)]\tLoss: 0.770917\n",
      "Train Epoch: 2 [1500/3103 (48%)]\tLoss: 0.631123\n",
      "Train Epoch: 2 [2000/3103 (63%)]\tLoss: 0.761894\n",
      "Train Epoch: 2 [2500/3103 (79%)]\tLoss: 0.734278\n",
      "Train Epoch: 2 [3000/3103 (95%)]\tLoss: 0.643194\n",
      "Train Epoch: 3 [0/3103 (0%)]\tLoss: 0.627267\n",
      "Train Epoch: 3 [500/3103 (16%)]\tLoss: 0.663227\n",
      "Train Epoch: 3 [1000/3103 (32%)]\tLoss: 0.593806\n",
      "Train Epoch: 3 [1500/3103 (48%)]\tLoss: 0.592459\n",
      "Train Epoch: 3 [2000/3103 (63%)]\tLoss: 0.625980\n",
      "Train Epoch: 3 [2500/3103 (79%)]\tLoss: 0.685499\n",
      "Train Epoch: 3 [3000/3103 (95%)]\tLoss: 0.620714\n",
      "Train Epoch: 4 [0/3103 (0%)]\tLoss: 0.633609\n",
      "Train Epoch: 4 [500/3103 (16%)]\tLoss: 0.621514\n",
      "Train Epoch: 4 [1000/3103 (32%)]\tLoss: 0.528019\n",
      "Train Epoch: 4 [1500/3103 (48%)]\tLoss: 0.515883\n",
      "Train Epoch: 4 [2000/3103 (63%)]\tLoss: 0.622404\n",
      "Train Epoch: 4 [2500/3103 (79%)]\tLoss: 0.530083\n",
      "Train Epoch: 4 [3000/3103 (95%)]\tLoss: 0.564679\n",
      "Train Epoch: 5 [0/3103 (0%)]\tLoss: 0.460687\n",
      "Train Epoch: 5 [500/3103 (16%)]\tLoss: 0.502674\n",
      "Train Epoch: 5 [1000/3103 (32%)]\tLoss: 0.463527\n",
      "Train Epoch: 5 [1500/3103 (48%)]\tLoss: 0.476310\n",
      "Train Epoch: 5 [2000/3103 (63%)]\tLoss: 0.418995\n",
      "Train Epoch: 5 [2500/3103 (79%)]\tLoss: 0.458918\n",
      "Train Epoch: 5 [3000/3103 (95%)]\tLoss: 0.462377\n",
      "Train Epoch: 6 [0/3103 (0%)]\tLoss: 0.450813\n",
      "Train Epoch: 6 [500/3103 (16%)]\tLoss: 0.453530\n",
      "Train Epoch: 6 [1000/3103 (32%)]\tLoss: 0.440187\n",
      "Train Epoch: 6 [1500/3103 (48%)]\tLoss: 0.471780\n",
      "Train Epoch: 6 [2000/3103 (63%)]\tLoss: 0.422690\n",
      "Train Epoch: 6 [2500/3103 (79%)]\tLoss: 0.464980\n",
      "Train Epoch: 6 [3000/3103 (95%)]\tLoss: 0.428638\n",
      "Train Epoch: 7 [0/3103 (0%)]\tLoss: 0.373957\n",
      "Train Epoch: 7 [500/3103 (16%)]\tLoss: 0.458916\n",
      "Train Epoch: 7 [1000/3103 (32%)]\tLoss: 0.385365\n",
      "Train Epoch: 7 [1500/3103 (48%)]\tLoss: 0.346845\n",
      "Train Epoch: 7 [2000/3103 (63%)]\tLoss: 0.366992\n",
      "Train Epoch: 7 [2500/3103 (79%)]\tLoss: 0.360447\n",
      "Train Epoch: 7 [3000/3103 (95%)]\tLoss: 0.289583\n",
      "Train Epoch: 8 [0/3103 (0%)]\tLoss: 0.340585\n",
      "Train Epoch: 8 [500/3103 (16%)]\tLoss: 0.349887\n",
      "Train Epoch: 8 [1000/3103 (32%)]\tLoss: 0.287155\n",
      "Train Epoch: 8 [1500/3103 (48%)]\tLoss: 0.333814\n",
      "Train Epoch: 8 [2000/3103 (63%)]\tLoss: 0.304796\n",
      "Train Epoch: 8 [2500/3103 (79%)]\tLoss: 0.328532\n",
      "Train Epoch: 8 [3000/3103 (95%)]\tLoss: 0.325492\n",
      "Train Epoch: 9 [0/3103 (0%)]\tLoss: 0.303435\n",
      "Train Epoch: 9 [500/3103 (16%)]\tLoss: 0.307601\n",
      "Train Epoch: 9 [1000/3103 (32%)]\tLoss: 0.336201\n",
      "Train Epoch: 9 [1500/3103 (48%)]\tLoss: 0.313871\n",
      "Train Epoch: 9 [2000/3103 (63%)]\tLoss: 0.323843\n",
      "Train Epoch: 9 [2500/3103 (79%)]\tLoss: 0.234327\n",
      "Train Epoch: 9 [3000/3103 (95%)]\tLoss: 0.278096\n",
      "Train Epoch: 10 [0/3103 (0%)]\tLoss: 0.226111\n",
      "Train Epoch: 10 [500/3103 (16%)]\tLoss: 0.256195\n",
      "Train Epoch: 10 [1000/3103 (32%)]\tLoss: 0.258857\n",
      "Train Epoch: 10 [1500/3103 (48%)]\tLoss: 0.253456\n",
      "Train Epoch: 10 [2000/3103 (63%)]\tLoss: 0.267238\n",
      "Train Epoch: 10 [2500/3103 (79%)]\tLoss: 0.253092\n",
      "Train Epoch: 10 [3000/3103 (95%)]\tLoss: 0.210465\n",
      "Train Epoch: 11 [0/3103 (0%)]\tLoss: 0.217181\n",
      "Train Epoch: 11 [500/3103 (16%)]\tLoss: 0.223188\n",
      "Train Epoch: 11 [1000/3103 (32%)]\tLoss: 0.231107\n",
      "Train Epoch: 11 [1500/3103 (48%)]\tLoss: 0.233993\n",
      "Train Epoch: 11 [2000/3103 (63%)]\tLoss: 0.225564\n",
      "Train Epoch: 11 [2500/3103 (79%)]\tLoss: 0.228364\n",
      "Train Epoch: 11 [3000/3103 (95%)]\tLoss: 0.221928\n",
      "Train Epoch: 12 [0/3103 (0%)]\tLoss: 0.173278\n",
      "Train Epoch: 12 [500/3103 (16%)]\tLoss: 0.189791\n",
      "Train Epoch: 12 [1000/3103 (32%)]\tLoss: 0.177110\n",
      "Train Epoch: 12 [1500/3103 (48%)]\tLoss: 0.198708\n",
      "Train Epoch: 12 [2000/3103 (63%)]\tLoss: 0.201622\n",
      "Train Epoch: 12 [2500/3103 (79%)]\tLoss: 0.223998\n",
      "Train Epoch: 12 [3000/3103 (95%)]\tLoss: 0.166322\n",
      "Train Epoch: 13 [0/3103 (0%)]\tLoss: 0.218248\n",
      "Train Epoch: 13 [500/3103 (16%)]\tLoss: 0.197383\n",
      "Train Epoch: 13 [1000/3103 (32%)]\tLoss: 0.186318\n",
      "Train Epoch: 13 [1500/3103 (48%)]\tLoss: 0.173359\n",
      "Train Epoch: 13 [2000/3103 (63%)]\tLoss: 0.197724\n",
      "Train Epoch: 13 [2500/3103 (79%)]\tLoss: 0.177754\n",
      "Train Epoch: 13 [3000/3103 (95%)]\tLoss: 0.187624\n",
      "Train Epoch: 14 [0/3103 (0%)]\tLoss: 0.175444\n",
      "Train Epoch: 14 [500/3103 (16%)]\tLoss: 0.150786\n",
      "Train Epoch: 14 [1000/3103 (32%)]\tLoss: 0.161061\n",
      "Train Epoch: 14 [1500/3103 (48%)]\tLoss: 0.171403\n",
      "Train Epoch: 14 [2000/3103 (63%)]\tLoss: 0.140753\n",
      "Train Epoch: 14 [2500/3103 (79%)]\tLoss: 0.172715\n",
      "Train Epoch: 14 [3000/3103 (95%)]\tLoss: 0.166335\n",
      "\n",
      "Test set: Average loss: 0.0226\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_nn(batch_size=50, learning_rate=0.001, epochs=15,\n",
    "              log_interval=10):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(208, 1024)\n",
    "            self.fc2 = nn.Linear(1024, 512)\n",
    "            self.fc3 = nn.Linear(512, 256)\n",
    "            self.fc4 = nn.Linear(256, 128)\n",
    "            self.fc5 = nn.Linear(128, 72)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = self.fc5(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    # create a loss function\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    # run a test loop\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        net_out = net(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "#         pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "#         correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_opt = 2\n",
    "    if run_opt == 1:\n",
    "        simple_gradient()\n",
    "    elif run_opt == 2:\n",
    "        create_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
