{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "from net import Net\n",
    "from enum import Enum\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from landmarksDataset import LandmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modes(Enum):\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataByMinMax(data, mins=None, maxs=None):\n",
    "    min_history = []\n",
    "    max_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        if (maxs is None or mins is None):\n",
    "            min_val = torch.min(data[:, i])\n",
    "            max_val = torch.max(data[:, i])\n",
    "        else:\n",
    "            min_val = mins[i]\n",
    "            max_val = maxs[i]\n",
    "\n",
    "        denominator = max_val - min_val\n",
    "        denominator = 1e-7 if denominator == 0 else denominator\n",
    "\n",
    "        # Save the min and max history for denormalize later\n",
    "        min_history.append(min_val)\n",
    "        max_history.append(max_val)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - min_val) / denominator\n",
    "        \n",
    "    history = (min_history, max_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataByStdMean(data, stds=None, means=None):\n",
    "    std_history = []\n",
    "    mean_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        if (stds is None or means is None):\n",
    "            std = torch.std(data[:, i])\n",
    "            mean = torch.mean(data[:, i])\n",
    "        else:\n",
    "            std = stds[i]\n",
    "            mean = means[i]\n",
    "\n",
    "        std = 1e-7 if std == 0 else std\n",
    "\n",
    "        # Save the std and mean history for denormalize later\n",
    "        std_history.append(std)\n",
    "        mean_history.append(mean)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - mean) / std\n",
    "        \n",
    "    history = (std_history, mean_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(mode, file, start, end):\n",
    "    return pd.read_csv(f'{config.samples_path}\\\\{mode}\\\\{file}',\n",
    "                               usecols = range(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode, device, x_history=None, y_history=None):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(mode, config.x_data_file, \n",
    "                        config.x_cols_start_index, config.x_cols_end_index)\n",
    "                        \n",
    "    y = getDataFromCSV(mode, config.y_data_file, \n",
    "                        config.y_cols_start_index, config.y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True, device = device).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True, device = device).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor, x_history = normalizeDataByMinMax(x_tensor, x_history[0] if x_history is not None else None, x_history[1] if x_history is not None else None)\n",
    "    norm_y_tensor, y_history = normalizeDataByMinMax(y_tensor, y_history[0] if y_history is not None else None, y_history[1] if y_history is not None else None)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor, x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(mode, device, x_history=None, y_history=None):\n",
    "    x, y, x_history, y_history = getData(mode, device, x_history, y_history)\n",
    "    # if (mode == Modes.TRAIN.value):\n",
    "    #     print(x.min(axis=0), x.max(axis=0), x.mean(axis=0), x.std(axis=0))\n",
    "    return LandmarksDataset(x, y), x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, epochs, train_loader, net, optimizer, criterion, log_interval, test_loader):\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        start = time()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #                100. * batch_idx / len(train_loader), epoch_loss.item() / (batch_idx+1)))\n",
    "        _, test_loss = test(device, test_loader ,net ,criterion)\n",
    "        print(\"train all dataset losss: \", epoch_loss.item())\n",
    "        print(\"division by number of examples: \", len(train_loader.dataset))\n",
    "        print(f'Epoch {epoch + 1} summary- train: {(epoch_loss.item() / len(train_loader.dataset)) * 100:.5f} test: {test_loss :.5f} took {time()-start:.2f}secs')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, test_loader, net, criterion):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        # correct = 0\n",
    "        results = torch.tensor([], device = device).float()\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            net_out = net(data)\n",
    "            results = torch.cat((results, net_out))\n",
    "            # sum up batch loss\n",
    "            loss = criterion(net_out, target)\n",
    "            test_loss += loss\n",
    "            # pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "            # correct += pred.eq(target.data).sum()\n",
    "            \n",
    "#         print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "#          test_loss))\n",
    "\n",
    "    print(\"test all dataset losss: \", test_loss.item())\n",
    "    print(\"division by number of examples: \", len(test_loader.dataset))\n",
    "    return results,  (test_loss.item() / len(test_loader.dataset)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSize(data, target):\n",
    "    # Convert tensors to numpy\n",
    "    data, target = data.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
    "    return data.shape[0], target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeDataByMinMax(data, min_history, max_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        min_val, max_val = min_history[i], max_history[i]\n",
    "        data[:, i] = (data[:, i] * (max_val - min_val)) + min_val\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeDataByStdMean(data, std_history, mean_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        std, mean = std_history[i], mean_history[i]\n",
    "        data[:, i] = (data[:, i] * std) + mean\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClipAndFrameCols(results):\n",
    "    clips = []\n",
    "    frames = []\n",
    "    loop_num = results.shape[0] // config.frame_num\n",
    "    remaining_div = results.shape[0] % config.frame_num\n",
    "\n",
    "    for i in range(loop_num):\n",
    "        clips = np.concatenate((clips, np.full((1, config.frame_num), str(i))), axis=None)\n",
    "        # clips = np.concatenate((clips, list(str(i) for i in range(config.frame_num))), axis=None)\n",
    "        str_num_range = [*map(str, range(config.frame_num))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "\n",
    "    # Add the remaining rows, if the csv file not contains exactly rows num that divide by frame_num   \n",
    "    if (remaining_div != 0):\n",
    "        clips = np.concatenate((clips, np.full((1, remaining_div), str(loop_num))), axis=None)\n",
    "        # clips = np.concatenate((clips, list(str(i) for i in range(remaining_div))), axis=None)\n",
    "        str_num_range = [*map(str, range(remaining_div))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "    \n",
    "    # Insert the clip & frame data to the results\n",
    "    results = np.insert(results, 0, clips, axis=1)\n",
    "    results = np.insert(results, 1, frames, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsToOutputFile(mode, filename, results):\n",
    "    # Create the output csv columns\n",
    "    blend_cols = []\n",
    "    clip_and_frame_cols = ['clip', 'frame']\n",
    "    quat_cols = ['Quaternion_x', 'Quaternion_y', 'Quaternion_z', 'Quaternion_w']\n",
    "    for i in range(config.blend_range[0], config.blend_range[1]):\n",
    "        blend_cols.append('Blendshape_{0}'.format(i))\n",
    "    output_cols = clip_and_frame_cols + quat_cols + blend_cols\n",
    "    \n",
    "    # Convert the results to from tensor to numpy \n",
    "    results = results.cpu().detach().numpy()\n",
    "\n",
    "    # Generate and insert clip & frame columns\n",
    "    results = generateClipAndFrameCols(results)\n",
    "\n",
    "    # Convert the results to data frame\n",
    "    results = pd.DataFrame(results, columns=output_cols)\n",
    "\n",
    "    # Convert columns data type from float to int\n",
    "    results['clip'] = results['clip'].astype(int)\n",
    "    results['frame'] = results['frame'].astype(int)\n",
    "    \n",
    "    # Assign values outside boundary to boundary values\n",
    "    results.loc[:, quat_cols] = results.loc[:, quat_cols].clip(config.quat_domain[0], config.quat_domain[1])\n",
    "    results.loc[:, blend_cols] = results.loc[:, blend_cols].clip(config.blend_domain[0], config.blend_domain[1])\n",
    "\n",
    "    # Save the results to the output csv file\n",
    "    file_path = f'{config.samples_path}\\\\{mode}\\\\{filename}'\n",
    "    results.to_csv(file_path, index=False)\n",
    "    print(f'[{datetime.now()}] Successfully saved the results to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=208, out_features=2048, bias=True)\n",
      "  (1): PReLU(num_parameters=1)\n",
      "  (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (4): PReLU(num_parameters=1)\n",
      "  (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Dropout(p=0.3, inplace=False)\n",
      "  (7): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (8): PReLU(num_parameters=1)\n",
      "  (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): Dropout(p=0.3, inplace=False)\n",
      "  (11): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (12): PReLU(num_parameters=1)\n",
      "  (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): Dropout(p=0.3, inplace=False)\n",
      "  (15): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (16): PReLU(num_parameters=1)\n",
      "  (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (18): Dropout(p=0.3, inplace=False)\n",
      "  (19): Linear(in_features=128, out_features=72, bias=True)\n",
      "  (20): ReLU()\n",
      ")\n",
      "test all dataset losss:  5070.96337890625\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  41584.81640625\n",
      "division by number of examples:  21135\n",
      "Epoch 1 summary- train: 196.75806 test: 163.42131 took 67.58secs\n",
      "test all dataset losss:  5196.98583984375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  36058.83203125\n",
      "division by number of examples:  21135\n",
      "Epoch 2 summary- train: 170.61193 test: 167.48262 took 17.98secs\n",
      "test all dataset losss:  5013.72119140625\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35802.03515625\n",
      "division by number of examples:  21135\n",
      "Epoch 3 summary- train: 169.39690 test: 161.57658 took 18.24secs\n",
      "test all dataset losss:  5007.94287109375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35585.5625\n",
      "division by number of examples:  21135\n",
      "Epoch 4 summary- train: 168.37266 test: 161.39036 took 18.00secs\n",
      "test all dataset losss:  5019.06787109375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35472.75390625\n",
      "division by number of examples:  21135\n",
      "Epoch 5 summary- train: 167.83891 test: 161.74888 took 17.97secs\n",
      "test all dataset losss:  4989.3876953125\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35436.8359375\n",
      "division by number of examples:  21135\n",
      "Epoch 6 summary- train: 167.66897 test: 160.79238 took 17.99secs\n",
      "test all dataset losss:  4992.02734375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35434.40625\n",
      "division by number of examples:  21135\n",
      "Epoch 7 summary- train: 167.65747 test: 160.87745 took 17.97secs\n",
      "test all dataset losss:  4998.01123046875\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35424.90234375\n",
      "division by number of examples:  21135\n",
      "Epoch 8 summary- train: 167.61250 test: 161.07029 took 17.95secs\n",
      "test all dataset losss:  5002.24658203125\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35407.984375\n",
      "division by number of examples:  21135\n",
      "Epoch 9 summary- train: 167.53246 test: 161.20679 took 17.95secs\n",
      "test all dataset losss:  5007.32177734375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35415.53125\n",
      "division by number of examples:  21135\n",
      "Epoch 10 summary- train: 167.56816 test: 161.37034 took 17.95secs\n",
      "test all dataset losss:  4991.1162109375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35413.85546875\n",
      "division by number of examples:  21135\n",
      "Epoch 11 summary- train: 167.56023 test: 160.84809 took 17.93secs\n",
      "test all dataset losss:  4991.8515625\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35421.9296875\n",
      "division by number of examples:  21135\n",
      "Epoch 12 summary- train: 167.59844 test: 160.87179 took 17.96secs\n",
      "test all dataset losss:  4991.60498046875\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35411.23828125\n",
      "division by number of examples:  21135\n",
      "Epoch 13 summary- train: 167.54785 test: 160.86384 took 17.98secs\n",
      "test all dataset losss:  5003.693359375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35413.48046875\n",
      "division by number of examples:  21135\n",
      "Epoch 14 summary- train: 167.55846 test: 161.25341 took 18.07secs\n",
      "test all dataset losss:  5012.42431640625\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35416.390625\n",
      "division by number of examples:  21135\n",
      "Epoch 15 summary- train: 167.57223 test: 161.53478 took 17.96secs\n",
      "test all dataset losss:  4995.47607421875\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35420.9609375\n",
      "division by number of examples:  21135\n",
      "Epoch 16 summary- train: 167.59385 test: 160.98859 took 18.00secs\n",
      "test all dataset losss:  5011.12841796875\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35426.69140625\n",
      "division by number of examples:  21135\n",
      "Epoch 17 summary- train: 167.62097 test: 161.49302 took 17.95secs\n",
      "test all dataset losss:  4996.67724609375\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35421.28515625\n",
      "division by number of examples:  21135\n",
      "Epoch 18 summary- train: 167.59539 test: 161.02730 took 17.97secs\n",
      "test all dataset losss:  5004.103515625\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35416.04296875\n",
      "division by number of examples:  21135\n",
      "Epoch 19 summary- train: 167.57058 test: 161.26663 took 18.01secs\n",
      "test all dataset losss:  5007.7158203125\n",
      "division by number of examples:  3103\n",
      "train all dataset losss:  35421.0703125\n",
      "division by number of examples:  21135\n",
      "Epoch 20 summary- train: 167.59437 test: 161.38304 took 18.60secs\n",
      "test all dataset losss:  5007.7158203125\n",
      "division by number of examples:  3103\n",
      "[2020-08-28 12:16:13.697764] Successfully saved the results to .\\samples\\test\\expressions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs=20\n",
    "    batch_size=50\n",
    "    log_interval=10\n",
    "    learning_rate=1e-3\n",
    "    \n",
    "    device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "    # Create the train data loader\n",
    "    train_dataset, x_history, y_history = getDataset(Modes.TRAIN.value, device)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "    # Create the test data loader\n",
    "    test_dataset, _, _ = getDataset(Modes.TEST.value, device, x_history, y_history)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    # Extract the input and output num of features\n",
    "    in_num_of_features, out_num_of_features = getFeatureSize(*train_dataset[0])\n",
    "\n",
    "    # Create the net\n",
    "    net = Net(in_num_of_features, out_num_of_features)\n",
    "    net.to(device)\n",
    "    print(net)\n",
    "\n",
    "    # Create an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a loss function\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    # Run the main training loop\n",
    "    net = train(device, epochs, train_loader, net, optimizer, criterion, log_interval, test_loader)\n",
    "                \n",
    "    # Run a test loop\n",
    "    results, test_loss = test(device, test_loader, net, criterion)\n",
    "    \n",
    "    # De-normalize the data to the original domains\n",
    "    results = denormalizeDataByMinMax(results, *y_history)\n",
    "    \n",
    "    # Save the results to output file\n",
    "    saveResultsToOutputFile(Modes.TEST.value, config.output_filename, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
