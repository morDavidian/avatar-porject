{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "from net import Net\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from landmarksDataset import LandmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modes(Enum):\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(data):\n",
    "    min_history = []\n",
    "    max_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        min_val = torch.min(data[:, i])\n",
    "        max_val = torch.max(data[:, i])\n",
    "\n",
    "        denominator = max_val - min_val\n",
    "        denominator = 0.001 if denominator == 0 else denominator\n",
    "\n",
    "        # Save the min and max history for denormalize later\n",
    "        min_history.append(min_val)\n",
    "        max_history.append(max_val)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - min_val) / denominator\n",
    "        \n",
    "    history = (min_history, max_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(mode, file, start, end):\n",
    "    return pd.read_csv(f'{config.samples_path}\\\\{mode}\\\\{file}',\n",
    "                               usecols = range(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(mode, config.x_data_file, \n",
    "                        config.x_cols_start_index, config.x_cols_end_index)\n",
    "                        \n",
    "    y = getDataFromCSV(mode, config.y_data_file, \n",
    "                        config.y_cols_start_index, config.y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor, _ = normalizeData(x_tensor)\n",
    "    norm_y_tensor, history = normalizeData(y_tensor)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(mode):\n",
    "    x, y, history = getData(mode)\n",
    "    return LandmarksDataset(x, y), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_loader, net, optimizer, criterion, log_interval):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net, criterion):\n",
    "    test_loss = 0\n",
    "    # correct = 0\n",
    "    results = torch.tensor([]).float()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        net_out = net(data)\n",
    "        results = torch.cat((results, net_out))\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "        # pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        # correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))\n",
    "\n",
    "    return results, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSize(data, target):\n",
    "    # Convert tensors to numpy\n",
    "    data, target = data.detach().numpy(), target.detach().numpy()\n",
    "    return data.shape[0], target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeData(data, min_history, max_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        min_val, max_val = min_history[i], max_history[i]\n",
    "        data[:, i] = (data[:, i] * (max_val - min_val)) + min_val\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClipAndFrameCols(results):\n",
    "    clips = []\n",
    "    frames = []\n",
    "    loop_num = results.shape[0] // config.frame_num\n",
    "    remaining_div = results.shape[0] % config.frame_num\n",
    "\n",
    "    for i in range(loop_num):\n",
    "        # clips = np.concatenate((clips, np.full((1, config.frame_num), str(i))), axis=None)\n",
    "        clips = np.concatenate((clips, list(str(i) for i in range(config.frame_num))), axis=None)\n",
    "        str_num_range = [*map(str, range(config.frame_num))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "\n",
    "    # Add the remaining rows, if the csv file not contains exactly rows num that divide by frame_num   \n",
    "    if (remaining_div != 0):\n",
    "        # clips = np.concatenate((clips, np.full((1, remaining_div), str(loop_num))), axis=None)\n",
    "        clips = np.concatenate((clips, list(str(i) for i in range(remaining_div))), axis=None)\n",
    "        str_num_range = [*map(str, range(remaining_div))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "    \n",
    "    # Insert the clip & frame data to the results\n",
    "    results = np.insert(results, 0, clips, axis=1)\n",
    "    results = np.insert(results, 1, frames, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsToOutputFile(mode, filename, results):\n",
    "    # Create the output csv columns\n",
    "    blend_cols = []\n",
    "    clip_and_frame_cols = ['clip', 'frame']\n",
    "    quat_cols = ['Quaternion_x', 'Quaternion_y', 'Quaternion_z', 'Quaternion_w']\n",
    "    for i in range(config.blend_range[0], config.blend_range[1]):\n",
    "        blend_cols.append('Blendshape_{0}'.format(i))\n",
    "    output_cols = clip_and_frame_cols + quat_cols + blend_cols\n",
    "    \n",
    "    # Convert the results to from tensor to numpy \n",
    "    results = results.detach().numpy()\n",
    "\n",
    "    # Generate and insert clip & frame columns\n",
    "    results = generateClipAndFrameCols(results)\n",
    "\n",
    "    # Convert the results to data frame\n",
    "    results = pd.DataFrame(results, columns=output_cols)\n",
    "\n",
    "    # Convert columns data type from float to int\n",
    "    results['clip'] = results['clip'].astype(int)\n",
    "    results['frame'] = results['frame'].astype(int)\n",
    "    \n",
    "    # Assign values outside boundary to boundary values\n",
    "    results.loc[:, quat_cols] = results.loc[:, quat_cols].clip(config.quat_domain[0], config.quat_domain[1])\n",
    "    results.loc[:, blend_cols] = results.loc[:, blend_cols].clip(config.blend_domain[0], config.blend_domain[1])\n",
    "\n",
    "    # Save the results to the output csv file\n",
    "    file_path = f'{config.samples_path}\\\\{mode}\\\\{filename}'\n",
    "    results.to_csv(file_path, index=False)\n",
    "    print(f'[{datetime.now()}] Successfully saved the results to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(batch_size=50, learning_rate=0.001, epochs=10,\n",
    "              log_interval=10):\n",
    "    \n",
    "    # Create the train data loader\n",
    "    train_dataset, _ = getDataset(Modes.TRAIN.value)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # Create the test data loader\n",
    "    test_dataset, history = getDataset(Modes.TEST.value)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    # Extract the input and output num of features\n",
    "    in_num_of_features, out_num_of_features = getFeatureSize(*train_dataset[0])\n",
    "\n",
    "    # Create the net\n",
    "    net = Net(in_num_of_features, out_num_of_features)\n",
    "\n",
    "    # Create an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a loss function\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    # Run the main training loop\n",
    "    net = train(epochs, train_loader, net, optimizer, criterion, log_interval)\n",
    "                \n",
    "    # Run a test loop\n",
    "    results, test_loss = test(test_loader, net, criterion)\n",
    "    \n",
    "    # De-normalize the data to the original domains\n",
    "    results = denormalizeData(results, *history)\n",
    "    \n",
    "    # Save the results to output file\n",
    "    saveResultsToOutputFile(Modes.TEST.value, config.output_filename, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}