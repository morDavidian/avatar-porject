{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "from net import Net\n",
    "from enum import Enum\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from landmarksDataset import LandmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modes(Enum):\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(data, mins=None, maxs=None):\n",
    "    min_history = []\n",
    "    max_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        if (maxs is None or mins is None):\n",
    "            min_val = torch.min(data[:, i])\n",
    "            max_val = torch.max(data[:, i])\n",
    "        else:\n",
    "            min_val = mins[i]\n",
    "            max_val = maxs[i]\n",
    "\n",
    "        denominator = max_val - min_val\n",
    "        denominator = 1e-7 if denominator == 0 else denominator\n",
    "\n",
    "        # Save the min and max history for denormalize later\n",
    "        min_history.append(min_val)\n",
    "        max_history.append(max_val)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - min_val) / denominator\n",
    "        \n",
    "    history = (min_history, max_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(mode, file, start, end):\n",
    "    return pd.read_csv(f'{config.samples_path}\\\\{mode}\\\\{file}',\n",
    "                               usecols = range(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode, x_history=None, y_history=None):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(mode, config.x_data_file, \n",
    "                        config.x_cols_start_index, config.x_cols_end_index)\n",
    "                        \n",
    "    y = getDataFromCSV(mode, config.y_data_file, \n",
    "                        config.y_cols_start_index, config.y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor, x_history = normalizeData(x_tensor, mins=x_history[0] if x_history is not None else None, maxs=x_history[1] if x_history is not None else None)\n",
    "    norm_y_tensor, y_history = normalizeData(y_tensor, mins=y_history[0] if y_history is not None else None, maxs=y_history[1] if y_history is not None else None)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor, x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(mode, x_history=None, y_history=None):\n",
    "    x, y, x_history, y_history = getData(mode, x_history, y_history)\n",
    "    if (mode == Modes.TRAIN.value):\n",
    "        print(x.min(), x.max())\n",
    "    return LandmarksDataset(x, y), x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_loader, net, optimizer, criterion, log_interval, test_loader):\n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #                100. * batch_idx / len(train_loader), epoch_loss.item() / (batch_idx+1)))\n",
    "        _, test_loss = test(test_loader ,net ,criterion)\n",
    "        print(f'Epoch {epoch + 1} summary- train: {epoch_loss.item() / (batch_idx+1) :.5f} test: {test_loss :.5f} took {time()-start:.2f}secs')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net, criterion):\n",
    "    test_loss = 0\n",
    "    # correct = 0\n",
    "    results = torch.tensor([]).float()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        net_out = net(data)\n",
    "        results = torch.cat((results, net_out))\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "        # pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        # correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "  #  print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "  #      test_loss))\n",
    "\n",
    "    return results, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSize(data, target):\n",
    "    # Convert tensors to numpy\n",
    "    data, target = data.detach().numpy(), target.detach().numpy()\n",
    "    return data.shape[0], target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeData(data, min_history, max_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        min_val, max_val = min_history[i], max_history[i]\n",
    "        data[:, i] = (data[:, i] * (max_val - min_val)) + min_val\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClipAndFrameCols(results):\n",
    "    clips = []\n",
    "    frames = []\n",
    "    loop_num = results.shape[0] // config.frame_num\n",
    "    remaining_div = results.shape[0] % config.frame_num\n",
    "\n",
    "    for i in range(loop_num):\n",
    "        # clips = np.concatenate((clips, np.full((1, config.frame_num), str(i))), axis=None)\n",
    "        clips = np.concatenate((clips, list(str(i) for i in range(config.frame_num))), axis=None)\n",
    "        str_num_range = [*map(str, range(config.frame_num))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "\n",
    "    # Add the remaining rows, if the csv file not contains exactly rows num that divide by frame_num   \n",
    "    if (remaining_div != 0):\n",
    "        # clips = np.concatenate((clips, np.full((1, remaining_div), str(loop_num))), axis=None)\n",
    "        clips = np.concatenate((clips, list(str(i) for i in range(remaining_div))), axis=None)\n",
    "        str_num_range = [*map(str, range(remaining_div))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "    \n",
    "    # Insert the clip & frame data to the results\n",
    "    results = np.insert(results, 0, clips, axis=1)\n",
    "    results = np.insert(results, 1, frames, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsToOutputFile(mode, filename, results):\n",
    "    # Create the output csv columns\n",
    "    blend_cols = []\n",
    "    clip_and_frame_cols = ['clip', 'frame']\n",
    "    quat_cols = ['Quaternion_x', 'Quaternion_y', 'Quaternion_z', 'Quaternion_w']\n",
    "    for i in range(config.blend_range[0], config.blend_range[1]):\n",
    "        blend_cols.append('Blendshape_{0}'.format(i))\n",
    "    output_cols = clip_and_frame_cols + quat_cols + blend_cols\n",
    "    \n",
    "    # Convert the results to from tensor to numpy \n",
    "    results = results.detach().numpy()\n",
    "\n",
    "    # Generate and insert clip & frame columns\n",
    "    results = generateClipAndFrameCols(results)\n",
    "\n",
    "    # Convert the results to data frame\n",
    "    results = pd.DataFrame(results, columns=output_cols)\n",
    "\n",
    "    # Convert columns data type from float to int\n",
    "    results['clip'] = results['clip'].astype(int)\n",
    "    results['frame'] = results['frame'].astype(int)\n",
    "    \n",
    "    # Assign values outside boundary to boundary values\n",
    "    results.loc[:, quat_cols] = results.loc[:, quat_cols].clip(config.quat_domain[0], config.quat_domain[1])\n",
    "    results.loc[:, blend_cols] = results.loc[:, blend_cols].clip(config.blend_domain[0], config.blend_domain[1])\n",
    "\n",
    "    # Save the results to the output csv file\n",
    "    file_path = f'{config.samples_path}\\\\{mode}\\\\{filename}'\n",
    "    results.to_csv(file_path, index=False)\n",
    "    print(f'[{datetime.now()}] Successfully saved the results to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MinBackward1>) tensor(1., grad_fn=<MaxBackward1>)\n",
      "Net(\n",
      "  (output_layer): Linear(in_features=128, out_features=72, bias=True)\n",
      ")\n",
      "Epoch 1 summary- train: 0.04742 test: 0.02321 took 9.42secs\n",
      "Epoch 2 summary- train: 0.02265 test: 0.02180 took 8.85secs\n",
      "Epoch 3 summary- train: 0.02193 test: 0.02151 took 12.45secs\n",
      "Epoch 4 summary- train: 0.02168 test: 0.02135 took 16.00secs\n",
      "Epoch 5 summary- train: 0.02146 test: 0.02123 took 22.14secs\n",
      "Epoch 6 summary- train: 0.02130 test: 0.02111 took 21.94secs\n",
      "Epoch 7 summary- train: 0.02104 test: 0.02095 took 22.23secs\n",
      "Epoch 8 summary- train: 0.02080 test: 0.02081 took 22.48secs\n",
      "Epoch 9 summary- train: 0.02060 test: 0.02069 took 22.66secs\n",
      "Epoch 10 summary- train: 0.02037 test: 0.02058 took 22.58secs\n",
      "Epoch 11 summary- train: 0.02017 test: 0.02048 took 23.08secs\n",
      "Epoch 12 summary- train: 0.02001 test: 0.02043 took 22.76secs\n",
      "Epoch 13 summary- train: 0.01986 test: 0.02036 took 23.01secs\n",
      "Epoch 14 summary- train: 0.01973 test: 0.02032 took 22.92secs\n",
      "Epoch 15 summary- train: 0.01959 test: 0.02030 took 22.97secs\n",
      "Epoch 16 summary- train: 0.01949 test: 0.02026 took 22.98secs\n",
      "Epoch 17 summary- train: 0.01937 test: 0.02027 took 22.49secs\n",
      "Epoch 18 summary- train: 0.01931 test: 0.02026 took 22.29secs\n",
      "Epoch 19 summary- train: 0.01922 test: 0.02026 took 22.37secs\n",
      "Epoch 20 summary- train: 0.01915 test: 0.02027 took 22.36secs\n",
      "Epoch 21 summary- train: 0.01909 test: 0.02024 took 23.14secs\n",
      "Epoch 22 summary- train: 0.01904 test: 0.02026 took 23.14secs\n",
      "Epoch 23 summary- train: 0.01900 test: 0.02027 took 22.88secs\n",
      "Epoch 24 summary- train: 0.01895 test: 0.02030 took 23.03secs\n",
      "Epoch 25 summary- train: 0.01891 test: 0.02029 took 22.90secs\n",
      "Epoch 26 summary- train: 0.01889 test: 0.02037 took 22.89secs\n",
      "Epoch 27 summary- train: 0.01886 test: 0.02037 took 22.78secs\n",
      "Epoch 28 summary- train: 0.01881 test: 0.02042 took 23.23secs\n",
      "Epoch 29 summary- train: 0.01879 test: 0.02043 took 22.99secs\n",
      "Epoch 30 summary- train: 0.01876 test: 0.02040 took 23.15secs\n",
      "Epoch 31 summary- train: 0.01873 test: 0.02045 took 23.04secs\n",
      "Epoch 32 summary- train: 0.01872 test: 0.02048 took 22.63secs\n",
      "Epoch 33 summary- train: 0.01871 test: 0.02048 took 22.68secs\n",
      "Epoch 34 summary- train: 0.01871 test: 0.02049 took 22.85secs\n",
      "Epoch 35 summary- train: 0.01869 test: 0.02051 took 23.05secs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs=50\n",
    "    batch_size=128\n",
    "    log_interval=10\n",
    "    learning_rate=1e-3\n",
    "    \n",
    "    # Create the train data loader\n",
    "    train_dataset, x_history, y_history = getDataset(Modes.TRAIN.value)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # Create the test data loader\n",
    "    test_dataset, _, _ = getDataset(Modes.TEST.value, x_history, y_history)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    # Extract the input and output num of features\n",
    "    in_num_of_features, out_num_of_features = getFeatureSize(*train_dataset[0])\n",
    "\n",
    "    # Create the net\n",
    "    net = Net(in_num_of_features, out_num_of_features)\n",
    "    print(net)\n",
    "\n",
    "    # Create an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a loss function\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    # Run the main training loop\n",
    "    net = train(epochs, train_loader, net, optimizer, criterion, log_interval, test_loader)\n",
    "                \n",
    "    # Run a test loop\n",
    "    results, test_loss = test(test_loader, net, criterion)\n",
    "    \n",
    "    # De-normalize the data to the original domains\n",
    "    results = denormalizeData(results, *y_history)\n",
    "    \n",
    "    # Save the results to output file\n",
    "    saveResultsToOutputFile(Modes.TEST.value, config.output_filename, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
