{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "from net import Net\n",
    "from enum import Enum\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from landmarksDataset import LandmarksDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modes(Enum):\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataByMinMax(data, mins=None, maxs=None):\n",
    "    min_history = []\n",
    "    max_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        if (maxs is None or mins is None):\n",
    "            min_val = torch.min(data[:, i])\n",
    "            max_val = torch.max(data[:, i])\n",
    "        else:\n",
    "            min_val = mins[i]\n",
    "            max_val = maxs[i]\n",
    "\n",
    "        denominator = max_val - min_val\n",
    "        denominator = 1e-7 if denominator == 0 else denominator\n",
    "\n",
    "        # Save the min and max history for denormalize later\n",
    "        min_history.append(min_val)\n",
    "        max_history.append(max_val)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - min_val) / denominator\n",
    "        \n",
    "    history = (min_history, max_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataByStdMean(data, stds=None, means=None):\n",
    "    std_history = []\n",
    "    mean_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        if (stds is None or means is None):\n",
    "            std = torch.std(data[:, i])\n",
    "            mean = torch.mean(data[:, i])\n",
    "        else:\n",
    "            std = stds[i]\n",
    "            mean = means[i]\n",
    "\n",
    "        std = 1e-7 if std == 0 else std\n",
    "\n",
    "        # Save the std and mean history for denormalize later\n",
    "        std_history.append(std)\n",
    "        mean_history.append(mean)\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - mean) / std\n",
    "        \n",
    "    history = (std_history, mean_history)\n",
    "    return data, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(mode, file, start, end):\n",
    "    return pd.read_csv(f'{config.samples_path}\\\\{mode}\\\\{file}',\n",
    "                               usecols = range(start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode, x_history=None, y_history=None):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(mode, config.x_data_file, \n",
    "                        config.x_cols_start_index, config.x_cols_end_index)\n",
    "                        \n",
    "    y = getDataFromCSV(mode, config.y_data_file, \n",
    "                        config.y_cols_start_index, config.y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True, device = device).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True, device = device).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor, x_history = normalizeDataByMinMax(x_tensor, x_history[0] if x_history is not None else None, x_history[1] if x_history is not None else None)\n",
    "    norm_y_tensor, y_history = normalizeDataByMinMax(y_tensor, y_history[0] if y_history is not None else None, y_history[1] if y_history is not None else None)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor, x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(mode, x_history=None, y_history=None):\n",
    "    x, y, x_history, y_history = getData(mode, x_history, y_history)\n",
    "    # if (mode == Modes.TRAIN.value):\n",
    "    #     print(x.min(axis=0), x.max(axis=0), x.mean(axis=0), x.std(axis=0))\n",
    "    return LandmarksDataset(x, y), x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_loader, net, optimizer, criterion, log_interval, test_loader):\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        start = time()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if batch_idx % log_interval == 0:\n",
    "            #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            #                100. * batch_idx / len(train_loader), epoch_loss.item() / (batch_idx+1)))\n",
    "        _, test_loss = test(test_loader ,net ,criterion)\n",
    "        print(f'Epoch {epoch + 1} summary- train: {(epoch_loss.item() / len(train_loader.dataset)) * 100:.5f} test: {test_loss :.5f} took {time()-start:.2f}secs')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net, criterion):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        # correct = 0\n",
    "        results = torch.tensor([], device = device).float()\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            net_out = net(data)\n",
    "            results = torch.cat((results, net_out))\n",
    "            # sum up batch loss\n",
    "            loss = criterion(net_out, target)\n",
    "            test_loss += loss\n",
    "            # pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "            # correct += pred.eq(target.data).sum()\n",
    "            \n",
    "#         print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "#          test_loss))\n",
    "\n",
    "    return results, (test_loss.item() / len(test_loader.dataset)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSize(data, target):\n",
    "    # Convert tensors to numpy\n",
    "    data, target = data.cpu().detach().numpy(), target.cpu().detach().numpy()\n",
    "    return data.shape[0], target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeDataByMinMax(data, min_history, max_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        min_val, max_val = min_history[i], max_history[i]\n",
    "        data[:, i] = (data[:, i] * (max_val - min_val)) + min_val\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeDataByStdMean(data, std_history, mean_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        std, mean = std_history[i], mean_history[i]\n",
    "        data[:, i] = (data[:, i] * std) + mean\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClipAndFrameCols(results):\n",
    "    clips = []\n",
    "    frames = []\n",
    "    loop_num = results.shape[0] // config.frame_num\n",
    "    remaining_div = results.shape[0] % config.frame_num\n",
    "\n",
    "    for i in range(loop_num):\n",
    "        clips = np.concatenate((clips, np.full((1, config.frame_num), str(i))), axis=None)\n",
    "        # clips = np.concatenate((clips, list(str(i) for i in range(config.frame_num))), axis=None)\n",
    "        str_num_range = [*map(str, range(config.frame_num))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "\n",
    "    # Add the remaining rows, if the csv file not contains exactly rows num that divide by frame_num   \n",
    "    if (remaining_div != 0):\n",
    "        clips = np.concatenate((clips, np.full((1, remaining_div), str(loop_num))), axis=None)\n",
    "        # clips = np.concatenate((clips, list(str(i) for i in range(remaining_div))), axis=None)\n",
    "        str_num_range = [*map(str, range(remaining_div))]\n",
    "        frames = np.concatenate((frames, str_num_range), axis=None)\n",
    "    \n",
    "    # Insert the clip & frame data to the results\n",
    "    results = np.insert(results, 0, clips, axis=1)\n",
    "    results = np.insert(results, 1, frames, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsToOutputFile(mode, filename, results):\n",
    "    # Create the output csv columns\n",
    "    blend_cols = []\n",
    "    clip_and_frame_cols = ['clip', 'frame']\n",
    "    quat_cols = ['Quaternion_x', 'Quaternion_y', 'Quaternion_z', 'Quaternion_w']\n",
    "    for i in range(config.blend_range[0], config.blend_range[1]):\n",
    "        blend_cols.append('Blendshape_{0}'.format(i))\n",
    "    output_cols = clip_and_frame_cols + quat_cols + blend_cols\n",
    "    \n",
    "    # Convert the results to from tensor to numpy \n",
    "    results = results.cpu().detach().numpy()\n",
    "\n",
    "    # Generate and insert clip & frame columns\n",
    "    results = generateClipAndFrameCols(results)\n",
    "\n",
    "    # Convert the results to data frame\n",
    "    results = pd.DataFrame(results, columns=output_cols)\n",
    "\n",
    "    # Convert columns data type from float to int\n",
    "    results['clip'] = results['clip'].astype(int)\n",
    "    results['frame'] = results['frame'].astype(int)\n",
    "    \n",
    "    # Assign values outside boundary to boundary values\n",
    "    results.loc[:, quat_cols] = results.loc[:, quat_cols].clip(config.quat_domain[0], config.quat_domain[1])\n",
    "    results.loc[:, blend_cols] = results.loc[:, blend_cols].clip(config.blend_domain[0], config.blend_domain[1])\n",
    "\n",
    "    # Save the results to the output csv file\n",
    "    file_path = f'{config.samples_path}\\\\{mode}\\\\{filename}'\n",
    "    results.to_csv(file_path, index=False)\n",
    "    print(f'[{datetime.now()}] Successfully saved the results to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device used is cuda\n",
      "Sequential(\n",
      "  (0): Linear(in_features=208, out_features=512, bias=True)\n",
      "  (1): PReLU(num_parameters=1)\n",
      "  (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (4): PReLU(num_parameters=1)\n",
      "  (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Dropout(p=0.3, inplace=False)\n",
      "  (7): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (8): PReLU(num_parameters=1)\n",
      "  (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): Dropout(p=0.3, inplace=False)\n",
      "  (11): Linear(in_features=128, out_features=72, bias=True)\n",
      "  (12): ReLU()\n",
      ")\n",
      "Epoch 1 summary- train: 314.68588 test: 243.21731 took 4.79secs\n",
      "Epoch 2 summary- train: 160.81665 test: 135.81059 took 4.89secs\n",
      "Epoch 3 summary- train: 143.68207 test: 128.73521 took 6.02secs\n",
      "Epoch 4 summary- train: 134.64342 test: 121.16467 took 6.46secs\n",
      "Epoch 5 summary- train: 129.06448 test: 119.71039 took 6.21secs\n",
      "Epoch 6 summary- train: 125.30889 test: 115.99995 took 6.09secs\n",
      "Epoch 7 summary- train: 123.20534 test: 119.74875 took 6.20secs\n",
      "Epoch 8 summary- train: 121.98815 test: 115.89289 took 6.07secs\n",
      "Epoch 9 summary- train: 120.54027 test: 118.27087 took 6.97secs\n",
      "Epoch 10 summary- train: 119.71967 test: 118.24046 took 7.45secs\n",
      "Epoch 11 summary- train: 119.08924 test: 113.39147 took 7.65secs\n",
      "Epoch 12 summary- train: 117.95811 test: 115.03943 took 7.56secs\n",
      "Epoch 13 summary- train: 117.65972 test: 114.81318 took 7.28secs\n",
      "Epoch 14 summary- train: 116.96636 test: 112.56077 took 7.39secs\n",
      "Epoch 15 summary- train: 116.34790 test: 114.56512 took 7.38secs\n",
      "Epoch 16 summary- train: 117.63871 test: 116.97902 took 7.54secs\n",
      "Epoch 17 summary- train: 116.86492 test: 116.48256 took 7.47secs\n",
      "Epoch 18 summary- train: 115.54939 test: 109.74037 took 7.50secs\n",
      "Epoch 19 summary- train: 114.91592 test: 111.44216 took 7.41secs\n",
      "Epoch 20 summary- train: 114.41896 test: 109.74949 took 8.28secs\n",
      "Epoch 21 summary- train: 114.43548 test: 111.98126 took 8.84secs\n",
      "Epoch 22 summary- train: 113.75201 test: 113.06949 took 7.73secs\n",
      "Epoch 23 summary- train: 113.73130 test: 119.64140 took 7.75secs\n",
      "Epoch 24 summary- train: 113.41698 test: 106.77847 took 7.62secs\n",
      "Epoch 25 summary- train: 112.68077 test: 109.76518 took 7.45secs\n",
      "Epoch 26 summary- train: 112.26417 test: 108.85378 took 19.60secs\n",
      "Epoch 27 summary- train: 111.89763 test: 111.09258 took 45.16secs\n",
      "Epoch 28 summary- train: 111.74635 test: 109.44320 took 44.56secs\n",
      "Epoch 29 summary- train: 111.35699 test: 112.54829 took 43.30secs\n",
      "Epoch 30 summary- train: 111.08211 test: 114.11786 took 43.38secs\n",
      "Epoch 31 summary- train: 110.55965 test: 130.24840 took 44.95secs\n",
      "Epoch 32 summary- train: 110.45707 test: 110.84287 took 44.70secs\n",
      "Epoch 33 summary- train: 110.16732 test: 105.78226 took 43.34secs\n",
      "Epoch 34 summary- train: 109.93709 test: 108.57978 took 45.44secs\n",
      "Epoch 35 summary- train: 109.58062 test: 105.85270 took 45.41secs\n",
      "Epoch 36 summary- train: 109.50812 test: 106.88840 took 46.07secs\n",
      "Epoch 37 summary- train: 109.13932 test: 105.23416 took 46.13secs\n",
      "Epoch 38 summary- train: 109.08839 test: 106.76278 took 46.09secs\n",
      "Epoch 39 summary- train: 108.82064 test: 106.53027 took 46.09secs\n",
      "Epoch 40 summary- train: 108.64557 test: 107.37879 took 45.98secs\n",
      "Epoch 41 summary- train: 108.40864 test: 119.06052 took 46.05secs\n",
      "Epoch 42 summary- train: 108.24947 test: 114.38525 took 46.05secs\n",
      "Epoch 43 summary- train: 108.12832 test: 112.28130 took 45.96secs\n",
      "Epoch 44 summary- train: 107.86223 test: 117.15010 took 46.06secs\n",
      "Epoch 45 summary- train: 107.83546 test: 105.23231 took 46.03secs\n",
      "Epoch 46 summary- train: 107.64029 test: 116.45749 took 46.06secs\n",
      "Epoch 47 summary- train: 107.50843 test: 110.79606 took 46.12secs\n",
      "Epoch 48 summary- train: 107.49301 test: 107.25954 took 46.08secs\n",
      "Epoch 49 summary- train: 107.34400 test: 109.70839 took 46.07secs\n",
      "Epoch 50 summary- train: 107.08094 test: 106.62769 took 46.06secs\n",
      "[2020-08-30 01:40:16.103237] Successfully saved the results to .\\samples\\test\\expressions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs=50\n",
    "    batch_size=256\n",
    "    log_interval=10\n",
    "    learning_rate=1e-3\n",
    "    \n",
    "    print(f'The device used is {device}')\n",
    "\n",
    "    # Create the train data loader\n",
    "    train_dataset, x_history, y_history = getDataset(Modes.TRAIN.value)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "    # Create the test data loader\n",
    "    test_dataset, _, _ = getDataset(Modes.TEST.value, x_history, y_history)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    # Extract the input and output num of features\n",
    "    in_num_of_features, out_num_of_features = getFeatureSize(*train_dataset[0])\n",
    "\n",
    "    # Create the net\n",
    "    net = Net(in_num_of_features, out_num_of_features)\n",
    "    net.to(device)\n",
    "    print(net)\n",
    "\n",
    "    # Create an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a loss function\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    # Run the main training loop\n",
    "    net = train(epochs, train_loader, net, optimizer, criterion, log_interval, test_loader)\n",
    "                \n",
    "    # Run a test loop\n",
    "    results, test_loss = test(test_loader, net, criterion)\n",
    "    \n",
    "    # De-normalize the data to the original domains\n",
    "    results = denormalizeDataByMinMax(results, *y_history)\n",
    "    \n",
    "    # Save the results to output file\n",
    "    saveResultsToOutputFile(Modes.TEST.value, config.output_filename, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
