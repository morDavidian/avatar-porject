{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as TF\n",
    "\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modes(Enum):\n",
    "    TRAIN = 'train'\n",
    "    TEST = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(mode, file, start, end):\n",
    "    return pd.read_csv(f'{config.samples_path}\\\\{mode}\\\\{file}',\n",
    "                               usecols = range(start, end))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normallizeData(data):\n",
    "    std_history = []\n",
    "    mean_history = []\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        # Compute standard deviation\n",
    "        std = torch.std(data[:, i])\n",
    "        std = 0.001 if std == 0 else std\n",
    "\n",
    "        # Save the std and mean history for denormalize later\n",
    "        std_history.append(std)\n",
    "        mean_history.append(torch.mean(data[:, i]))\n",
    "\n",
    "        # Normalize the data\n",
    "        data[:, i] = (data[:, i] - torch.mean(data[:, i])) / std\n",
    "        \n",
    "    return data, std_history, mean_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(mode, config.x_data_file, \n",
    "                        config.x_cols_start_index, config.x_cols_end_index)\n",
    "                        \n",
    "    y = getDataFromCSV(mode, config.y_data_file, \n",
    "                        config.y_cols_start_index, config.y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor, _, _ = normallizeData(x_tensor)\n",
    "    norm_y_tensor, y_std_history, y_mean_history = normallizeData(y_tensor)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor, y_std_history, y_mean_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(mode):\n",
    "    x, y, y_std_history, y_mean_history = getData(mode)\n",
    "    return LandmarksDataset(x, y), y_std_history, y_mean_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_num_of_features, out_num_of_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_num_of_features, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, out_num_of_features)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_loader, net, optimizer, criterion, log_interval):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net, criterion):\n",
    "    test_loss = 0\n",
    "    # correct = 0\n",
    "    results = torch.tensor([]).float()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        net_out = net(data)\n",
    "        results = torch.cat((results, net_out))\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "        # pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        # correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))\n",
    "\n",
    "    return results, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSize(data, target):\n",
    "    # Convert tensors to numpy\n",
    "    data, target = data.detach().numpy(), target.detach().numpy()\n",
    "    return data.shape[0], target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalizeData(data, std_history, mean_history):\n",
    "    for i in range(data.shape[1]):\n",
    "        std, mean = std_history[i], mean_history[i]\n",
    "        data[:, i] = (data[:, i] + mean) * std\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResultsToOutputFile(mode, filename, results):\n",
    "    # Create the output csv columns\n",
    "    blend_cols = []\n",
    "    quat_cols = ['Quaternion_x', 'Quaternion_y', 'Quaternion_z', 'Quaternion_w']\n",
    "    for i in range (config.blend_range[0], config.blend_range[1]):\n",
    "        blend_cols.append('Blendshape_{0}'.format(i))\n",
    "    output_cols = quat_cols + blend_cols\n",
    "    \n",
    "    # Convert the results to data frame \n",
    "    results = results.detach().numpy()\n",
    "    results = pd.DataFrame(results, columns=output_cols)\n",
    "    \n",
    "    # Assign values outside boundary to boundary values\n",
    "    results.loc[:, quat_cols] = results.loc[:, quat_cols].clip(config.quat_domain[0], config.quat_domain[1])\n",
    "    results.loc[:, blend_cols] = results.loc[:, blend_cols].clip(config.blend_domain[0], config.blend_domain[1])\n",
    "\n",
    "    # Save the results to the output csv file\n",
    "    file_path = f'{config.samples_path}\\\\{mode}\\\\{filename}'\n",
    "    results.to_csv(file_path)\n",
    "    print(f'[{datetime.now()}] Successfully saved the results to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn(batch_size=50, learning_rate=0.001, epochs=20,\n",
    "              log_interval=10):\n",
    "    \n",
    "    # Create the train data loader\n",
    "    train_dataset, _, _ = getDataset(Modes.TRAIN.value)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    # Create the test data loader\n",
    "    test_dataset, std_history, mean_history = getDataset(Modes.TEST.value)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    # Extract the input and output num of features\n",
    "    in_num_of_features, out_num_of_features = getFeatureSize(*train_dataset[0])\n",
    "\n",
    "    # Create the net\n",
    "    net = Net(in_num_of_features, out_num_of_features)\n",
    "\n",
    "    # Create an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a loss function\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "\n",
    "    # Run the main training loop\n",
    "    net = train(epochs, train_loader, net, optimizer, criterion, log_interval)\n",
    "                \n",
    "    # Run a test loop\n",
    "    results, test_loss = test(test_loader, net, criterion)\n",
    "    \n",
    "    # De-normalize the data to the original domains\n",
    "    results = denormalizeData(results, std_history, mean_history)\n",
    "    \n",
    "    # Save the results to output file\n",
    "    saveResultsToOutputFile(Modes.TEST.value, config.output_filename, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3269 (0%)]\tLoss: 1.021283\n",
      "Train Epoch: 0 [500/3269 (15%)]\tLoss: 0.971901\n",
      "Train Epoch: 0 [1000/3269 (30%)]\tLoss: 0.922604\n",
      "Train Epoch: 0 [1500/3269 (45%)]\tLoss: 0.960005\n",
      "Train Epoch: 0 [2000/3269 (61%)]\tLoss: 0.895052\n",
      "Train Epoch: 0 [2500/3269 (76%)]\tLoss: 0.982850\n",
      "Train Epoch: 0 [3000/3269 (91%)]\tLoss: 0.984340\n",
      "Train Epoch: 1 [0/3269 (0%)]\tLoss: 0.984140\n",
      "Train Epoch: 1 [500/3269 (15%)]\tLoss: 0.828951\n",
      "Train Epoch: 1 [1000/3269 (30%)]\tLoss: 0.781886\n",
      "Train Epoch: 1 [1500/3269 (45%)]\tLoss: 0.817600\n",
      "Train Epoch: 1 [2000/3269 (61%)]\tLoss: 0.890351\n",
      "Train Epoch: 1 [2500/3269 (76%)]\tLoss: 0.866007\n",
      "Train Epoch: 1 [3000/3269 (91%)]\tLoss: 0.725065\n",
      "Train Epoch: 2 [0/3269 (0%)]\tLoss: 0.675849\n",
      "Train Epoch: 2 [500/3269 (15%)]\tLoss: 0.653198\n",
      "Train Epoch: 2 [1000/3269 (30%)]\tLoss: 0.737489\n",
      "Train Epoch: 2 [1500/3269 (45%)]\tLoss: 0.665914\n",
      "Train Epoch: 2 [2000/3269 (61%)]\tLoss: 0.638704\n",
      "Train Epoch: 2 [2500/3269 (76%)]\tLoss: 0.677700\n",
      "Train Epoch: 2 [3000/3269 (91%)]\tLoss: 0.698022\n",
      "Train Epoch: 3 [0/3269 (0%)]\tLoss: 0.692729\n",
      "Train Epoch: 3 [500/3269 (15%)]\tLoss: 0.573448\n",
      "Train Epoch: 3 [1000/3269 (30%)]\tLoss: 0.578357\n",
      "Train Epoch: 3 [1500/3269 (45%)]\tLoss: 0.564795\n",
      "Train Epoch: 3 [2000/3269 (61%)]\tLoss: 0.597073\n",
      "Train Epoch: 3 [2500/3269 (76%)]\tLoss: 0.628065\n",
      "Train Epoch: 3 [3000/3269 (91%)]\tLoss: 0.562501\n",
      "Train Epoch: 4 [0/3269 (0%)]\tLoss: 0.491279\n",
      "Train Epoch: 4 [500/3269 (15%)]\tLoss: 0.479491\n",
      "Train Epoch: 4 [1000/3269 (30%)]\tLoss: 0.506323\n",
      "Train Epoch: 4 [1500/3269 (45%)]\tLoss: 0.530122\n",
      "Train Epoch: 4 [2000/3269 (61%)]\tLoss: 0.523913\n",
      "Train Epoch: 4 [2500/3269 (76%)]\tLoss: 0.487754\n",
      "Train Epoch: 4 [3000/3269 (91%)]\tLoss: 0.513989\n",
      "Train Epoch: 5 [0/3269 (0%)]\tLoss: 0.432451\n",
      "Train Epoch: 5 [500/3269 (15%)]\tLoss: 0.458919\n",
      "Train Epoch: 5 [1000/3269 (30%)]\tLoss: 0.454120\n",
      "Train Epoch: 5 [1500/3269 (45%)]\tLoss: 0.439594\n",
      "Train Epoch: 5 [2000/3269 (61%)]\tLoss: 0.429041\n",
      "Train Epoch: 5 [2500/3269 (76%)]\tLoss: 0.392691\n",
      "Train Epoch: 5 [3000/3269 (91%)]\tLoss: 0.398701\n",
      "Train Epoch: 6 [0/3269 (0%)]\tLoss: 0.372140\n",
      "Train Epoch: 6 [500/3269 (15%)]\tLoss: 0.317628\n",
      "Train Epoch: 6 [1000/3269 (30%)]\tLoss: 0.428424\n",
      "Train Epoch: 6 [1500/3269 (45%)]\tLoss: 0.337475\n",
      "Train Epoch: 6 [2000/3269 (61%)]\tLoss: 0.369481\n",
      "Train Epoch: 6 [2500/3269 (76%)]\tLoss: 0.338275\n",
      "Train Epoch: 6 [3000/3269 (91%)]\tLoss: 0.367478\n",
      "Train Epoch: 7 [0/3269 (0%)]\tLoss: 0.333972\n",
      "Train Epoch: 7 [500/3269 (15%)]\tLoss: 0.343372\n",
      "Train Epoch: 7 [1000/3269 (30%)]\tLoss: 0.364404\n",
      "Train Epoch: 7 [1500/3269 (45%)]\tLoss: 0.305315\n",
      "Train Epoch: 7 [2000/3269 (61%)]\tLoss: 0.296306\n",
      "Train Epoch: 7 [2500/3269 (76%)]\tLoss: 0.302130\n",
      "Train Epoch: 7 [3000/3269 (91%)]\tLoss: 0.282476\n",
      "Train Epoch: 8 [0/3269 (0%)]\tLoss: 0.314499\n",
      "Train Epoch: 8 [500/3269 (15%)]\tLoss: 0.223247\n",
      "Train Epoch: 8 [1000/3269 (30%)]\tLoss: 0.326847\n",
      "Train Epoch: 8 [1500/3269 (45%)]\tLoss: 0.272196\n",
      "Train Epoch: 8 [2000/3269 (61%)]\tLoss: 0.256904\n",
      "Train Epoch: 8 [2500/3269 (76%)]\tLoss: 0.282424\n",
      "Train Epoch: 8 [3000/3269 (91%)]\tLoss: 0.237444\n",
      "Train Epoch: 9 [0/3269 (0%)]\tLoss: 0.222813\n",
      "Train Epoch: 9 [500/3269 (15%)]\tLoss: 0.220620\n",
      "Train Epoch: 9 [1000/3269 (30%)]\tLoss: 0.237354\n",
      "Train Epoch: 9 [1500/3269 (45%)]\tLoss: 0.239301\n",
      "Train Epoch: 9 [2000/3269 (61%)]\tLoss: 0.243334\n",
      "Train Epoch: 9 [2500/3269 (76%)]\tLoss: 0.197355\n",
      "Train Epoch: 9 [3000/3269 (91%)]\tLoss: 0.253036\n",
      "Train Epoch: 10 [0/3269 (0%)]\tLoss: 0.249578\n",
      "Train Epoch: 10 [500/3269 (15%)]\tLoss: 0.215767\n",
      "Train Epoch: 10 [1000/3269 (30%)]\tLoss: 0.215118\n",
      "Train Epoch: 10 [1500/3269 (45%)]\tLoss: 0.234780\n",
      "Train Epoch: 10 [2000/3269 (61%)]\tLoss: 0.186764\n",
      "Train Epoch: 10 [2500/3269 (76%)]\tLoss: 0.238252\n",
      "Train Epoch: 10 [3000/3269 (91%)]\tLoss: 0.184803\n",
      "Train Epoch: 11 [0/3269 (0%)]\tLoss: 0.201746\n",
      "Train Epoch: 11 [500/3269 (15%)]\tLoss: 0.171824\n",
      "Train Epoch: 11 [1000/3269 (30%)]\tLoss: 0.175204\n",
      "Train Epoch: 11 [1500/3269 (45%)]\tLoss: 0.172470\n",
      "Train Epoch: 11 [2000/3269 (61%)]\tLoss: 0.197446\n",
      "Train Epoch: 11 [2500/3269 (76%)]\tLoss: 0.211374\n",
      "Train Epoch: 11 [3000/3269 (91%)]\tLoss: 0.170634\n",
      "Train Epoch: 12 [0/3269 (0%)]\tLoss: 0.193625\n",
      "Train Epoch: 12 [500/3269 (15%)]\tLoss: 0.208238\n",
      "Train Epoch: 12 [1000/3269 (30%)]\tLoss: 0.168909\n",
      "Train Epoch: 12 [1500/3269 (45%)]\tLoss: 0.190991\n",
      "Train Epoch: 12 [2000/3269 (61%)]\tLoss: 0.149447\n",
      "Train Epoch: 12 [2500/3269 (76%)]\tLoss: 0.147112\n",
      "Train Epoch: 12 [3000/3269 (91%)]\tLoss: 0.181766\n",
      "Train Epoch: 13 [0/3269 (0%)]\tLoss: 0.142763\n",
      "Train Epoch: 13 [500/3269 (15%)]\tLoss: 0.157662\n",
      "Train Epoch: 13 [1000/3269 (30%)]\tLoss: 0.186713\n",
      "Train Epoch: 13 [1500/3269 (45%)]\tLoss: 0.141657\n",
      "Train Epoch: 13 [2000/3269 (61%)]\tLoss: 0.159681\n",
      "Train Epoch: 13 [2500/3269 (76%)]\tLoss: 0.141498\n",
      "Train Epoch: 13 [3000/3269 (91%)]\tLoss: 0.137821\n",
      "Train Epoch: 14 [0/3269 (0%)]\tLoss: 0.117992\n",
      "Train Epoch: 14 [500/3269 (15%)]\tLoss: 0.139761\n",
      "Train Epoch: 14 [1000/3269 (30%)]\tLoss: 0.154847\n",
      "Train Epoch: 14 [1500/3269 (45%)]\tLoss: 0.125457\n",
      "Train Epoch: 14 [2000/3269 (61%)]\tLoss: 0.136776\n",
      "Train Epoch: 14 [2500/3269 (76%)]\tLoss: 0.114294\n",
      "Train Epoch: 14 [3000/3269 (91%)]\tLoss: 0.140821\n",
      "Train Epoch: 15 [0/3269 (0%)]\tLoss: 0.101723\n",
      "Train Epoch: 15 [500/3269 (15%)]\tLoss: 0.125350\n",
      "Train Epoch: 15 [1000/3269 (30%)]\tLoss: 0.123186\n",
      "Train Epoch: 15 [1500/3269 (45%)]\tLoss: 0.132064\n",
      "Train Epoch: 15 [2000/3269 (61%)]\tLoss: 0.128226\n",
      "Train Epoch: 15 [2500/3269 (76%)]\tLoss: 0.102696\n",
      "Train Epoch: 15 [3000/3269 (91%)]\tLoss: 0.132117\n",
      "Train Epoch: 16 [0/3269 (0%)]\tLoss: 0.111889\n",
      "Train Epoch: 16 [500/3269 (15%)]\tLoss: 0.097396\n",
      "Train Epoch: 16 [1000/3269 (30%)]\tLoss: 0.111316\n",
      "Train Epoch: 16 [1500/3269 (45%)]\tLoss: 0.103809\n",
      "Train Epoch: 16 [2000/3269 (61%)]\tLoss: 0.106181\n",
      "Train Epoch: 16 [2500/3269 (76%)]\tLoss: 0.129555\n",
      "Train Epoch: 16 [3000/3269 (91%)]\tLoss: 0.109411\n",
      "Train Epoch: 17 [0/3269 (0%)]\tLoss: 0.094848\n",
      "Train Epoch: 17 [500/3269 (15%)]\tLoss: 0.105512\n",
      "Train Epoch: 17 [1000/3269 (30%)]\tLoss: 0.089603\n",
      "Train Epoch: 17 [1500/3269 (45%)]\tLoss: 0.097038\n",
      "Train Epoch: 17 [2000/3269 (61%)]\tLoss: 0.094716\n",
      "Train Epoch: 17 [2500/3269 (76%)]\tLoss: 0.103250\n",
      "Train Epoch: 17 [3000/3269 (91%)]\tLoss: 0.108728\n",
      "Train Epoch: 18 [0/3269 (0%)]\tLoss: 0.088529\n",
      "Train Epoch: 18 [500/3269 (15%)]\tLoss: 0.100521\n",
      "Train Epoch: 18 [1000/3269 (30%)]\tLoss: 0.095156\n",
      "Train Epoch: 18 [1500/3269 (45%)]\tLoss: 0.109027\n",
      "Train Epoch: 18 [2000/3269 (61%)]\tLoss: 0.099795\n",
      "Train Epoch: 18 [2500/3269 (76%)]\tLoss: 0.104635\n",
      "Train Epoch: 18 [3000/3269 (91%)]\tLoss: 0.101725\n",
      "Train Epoch: 19 [0/3269 (0%)]\tLoss: 0.079312\n",
      "Train Epoch: 19 [500/3269 (15%)]\tLoss: 0.086844\n",
      "Train Epoch: 19 [1000/3269 (30%)]\tLoss: 0.102984\n",
      "Train Epoch: 19 [1500/3269 (45%)]\tLoss: 0.082875\n",
      "Train Epoch: 19 [2000/3269 (61%)]\tLoss: 0.076723\n",
      "Train Epoch: 19 [2500/3269 (76%)]\tLoss: 0.097630\n",
      "Train Epoch: 19 [3000/3269 (91%)]\tLoss: 0.081436\n",
      "\n",
      "Test set: Average loss: 0.0235\n",
      "[2020-08-22 20:53:59.168911] Successfully saved the results to .\\samples\\test\\expressions.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
