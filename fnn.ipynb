{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define global variables:\n",
    "samples_path = os.environ.get('SAMPLES_PATH') or '.\\\\samples'\n",
    "\n",
    "x_data_file = os.environ.get('X_DATA_FILE') or 'jupyter_landmarks.csv'\n",
    "x_cols_start_index = os.environ.get('X_COLS_START_INDEX') or 1\n",
    "x_cols_end_index = os.environ.get('X_COLS_END_INDEX') or 209\n",
    "\n",
    "y_data_file = os.environ.get('Y_DATA_FILE') or 'unity_blendshapes.csv'\n",
    "y_cols_start_index = os.environ.get('Y_COLS_START_INDEX') or 2\n",
    "y_cols_end_index = os.environ.get('Y_COLS_END_INDEX') or 74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromCSV(env, file, start, end):\n",
    "    return pd.read_csv(f'{samples_path}\\\\{env}\\\\{file}',\n",
    "                               usecols = range(start, end))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normallizeData(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        norm = torch.std(data[:, i])\n",
    "        norm = 0.001 if norm == 0 else norm\n",
    "        data[:, i] = (data[:, i] - torch.mean(data[:, i])) / norm\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(env):\n",
    "    # Get data from csv files\n",
    "    x = getDataFromCSV(env, x_data_file, x_cols_start_index, x_cols_end_index)\n",
    "    y = getDataFromCSV(env, y_data_file, y_cols_start_index, y_cols_end_index)\n",
    "\n",
    "    # Transforms the data to tensors\n",
    "    x_tensor = torch.tensor(x.values, requires_grad=True).float()\n",
    "    y_tensor = torch.tensor(y.values, requires_grad=True).float()\n",
    "\n",
    "    # Normallize the data\n",
    "    norm_x_tensor = normallizeData(x_tensor)\n",
    "    norm_y_tensor = normallizeData(y_tensor)\n",
    "\n",
    "    return norm_x_tensor, norm_y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(env):\n",
    "    x, y = getData(env)\n",
    "    return LandmarksDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_nn(batch_size=50, learning_rate=0.001, epochs=15,\n",
    "              log_interval=10):\n",
    "    \n",
    "    train_dataset = getDataset('train')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_dataset=getDataset('test')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(208, 1024)\n",
    "            self.fc2 = nn.Linear(1024, 512)\n",
    "            self.fc3 = nn.Linear(512, 256)\n",
    "            self.fc4 = nn.Linear(256, 128)\n",
    "            self.fc5 = nn.Linear(128, 72)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = self.fc5(x)\n",
    "            return x\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    # create a loss function\n",
    "    # criterion = nn.L1Loss(reduction='mean')\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    # run a test loop\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        net_out = net(data)\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "#         pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "#         correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_opt = 2\n",
    "    if run_opt == 1:\n",
    "        simple_gradient()\n",
    "    elif run_opt == 2:\n",
    "        create_nn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}